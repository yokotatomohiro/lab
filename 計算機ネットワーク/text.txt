計算機ネットワーク　論文要約
Spanner:Google's Globally-Distributed Database

Abstract
Spannerは, Googleのスケーラブルなマルチバージョンのグローバルに分散された同期複製データベースである.
グローバルな規模でデータを分散し, 外部整合性のある分散トランザクションをサポートする最初のシステムである.
このホワイトペーパーでは, Spannerの構造, その機能セット, 様々な設計決定の根拠, クロックの不確実性を明らかにする新しい時間APIについて説明する.
このAPIとその実装は, 外部の一貫性と様々な強力な機能をサポートするために重要である.
Spanner全体での過去の非ブロック読み取り, ロックフリー読み取り専用トランザクション, アトミックスキーマの変更. 
Spannerは, Googleで設計, 構築, 展開されたスケーラブルなグローバルに分散したデータベースである. 
最高レベルの抽象化では, 世界に広がるデータセンターの多くのPaxosステートマシンのセットにデータを分散するデータベースである.
レプリケーションは, グローバルな可用性と地理的な場所に使用される.
クライアントはレプリカ間で自動的にフェールオーバーする.
Spannerは, データの量またはサーバーの数が変化すると, マシン間でデータを自動的にリシャーディングし, マシン間(データセンター間を含む)でデータを自動的に移行して, 負荷と障害への対応を調整する.
Spannerは, 数百のデータセンターと数兆のデータベース行にわたって数百万台のマシンまで拡張できるように設計されている.

アプリケーションは, 大陸内または大陸間でデータを複製することにより, 広域の自然災害が発生した場合でも, Spannerを使用して高可用性を実現できる.
最初の顧客はGoogleの広告バックエンドを書き出したF1であった. 
F1は, 米国全体に広がる5つのレプリカを使用する. 
他のほとんどのアプリケーションは, おそらく1つの地理的地域の3~5つのデータセンターにデータを複製するが, 比較的独立した障害モードがある.
つまり, ほとんどのアプリケーションは, 1つまたは２つのデータセンター障害に耐えることができる限り, 高可用性よりも低遅延を選択する. 

Spannerの主な焦点は, データセンター間で複製されたデータの管理であるが, 分散システムインフラストラクチャ上で重要なデータベース機能の設計と実装にも多くの時間を費やした.
多くのプロジェクトが喜んでBigtableを使用しているが, Bigtableは特定の種類のアプリケーションで衣装するのが厳しいというユーザーからの苦情も一貫して受けている.
複雑で進化しているスキーマを持つもの, または広域レプリケーションの存在下で強力な一貫性を必要とするもの.
(同様の主張が他の著者によってなされている.)
Googleの多くのアプリケーションは, 書き込みスループットが比較的低いにも関わらず, 半リレーショナルデータモデルと同期レプリケーションのサポートのためにMegastoreを使用することを選択した.
この結果, SpannerはBigtableのようなバージョン管理されたキー値ストアから一時的なマルチバージョンデータベースに進化した.
データは, スキーマ化された相互関係テーブルに保存される.
データはバーション管理され, 各バージョンにはコミット時間のタイムスタンプが自動的に付けられる.
古いバージョンのデータは, 構成可能なガベージコレクションポリシーに従う.
アプリケーションは古いタイムスタンプでデータを読み取ることができる.
Spannerは汎用トランザクションをサポートし, SQLデータのクエリ言語を提供する.

グローバルに分散されたデータベースとして, Spannerはいくつかの興味深い機能を提供する.
第一に, データのレプリケーション構成は, アプリケーションによって動的にきめ細かく制御できる.
アプリケーションは制約を指定して, どのデータセンターにどのデータを含めるか, データがユーザーからどれだけ離れているか(読み取りレイテンシーを制御する), レプリカ同士がどれだけ離れているか(書き込みレイテンシーを制御する),維持するレプリカの数を制御できる(耐久性, 可用性, 読み取りパフォーマンスを制御するため).
また, システムはデータセンター間でデータを動的かつ透過的に移動して, データセンター間でリソース使用量のバランスを取ることができる.
第二に, Spannerには, 分散データベースに実装するのが厳しい２つの機能がある.
タイムスタンプ時にデータベース全体で外部的一貫した読み取りと書き込み, およびグローバルに一貫した読み取りを提供する.
これらの機能により, Spannerは一貫したバックアップ, 一貫したMapReduceの実行, およびアトミックスキーマの更新を, 全てグローバルな規模で, 進行中のトランザクションが存在する場合でもサポートできる.

これらの機能は, トランザクションが配信される場合でも, Spannerがグローバルに意味のあるコミットタイムスタンプをトランザクションに割り当てるという事実によって有効となる.
タイムスタンプはシリアル化の順序を反映している.
さらに, シリアル化の順序は, 外部整合性(または同等に, 線形化可能性)を満たしている.
トランザクションT1が別のトランザクションT2が開始する前にコミットする場合, T1のコミットタイムスタンプはT2のコミットタイムスタンプよりも小さくなる.
Spannerは, このような保証を世界規模で提供する最初のシステムである.

これらのプロパティの重要なイネーブラーは, 新しいTrueTime APIとその実装である.
APIはクロックの不確実性を直接公開し, Spannerのタイムスタンプの保証は実装が提供する境界に依存する.
不確実性が大きい場合, Spannerはその不確実性を持つためにスローダウンする.
Googleのクラスター管理ソフトウェアはTrueTime APIの実装を提供する.
この実装では, 複数の最新のクロックリファレンス(GPSおよびアトミッククロック)を使用することで, 不確実性を小さくしている(通常10ミリ秒未満).

二章では, Spannerの実装の構造, その機能セット, および設計に組み込まれたエンジニアリングの決定について説明する.
三章では, 新しいTrueTime APIについて説明し, その実装をスケッチする.
四章では, SpannerがTrueTimeを使用して外部正当性のある分散トランザクション, ロックフリー読み取り専用トランザクション, およびアトミックスキーマ更新を実装する方法について説明する.
五章では, SpannerのパフォーマンスとTrueTimeの動作に関するベンチマークを提供し, F1の経験について説明している.

2. Implementation
この章では, Spannerの実装の構造と論理的根拠について説明する.
次に, レプリケーションとローカリティの管理について説明する. これはデータ移動の単位である.
最後に, データモデル, Spannerがキーバリューストアではなくリレーショナルデータベースのように見える理由, およびアプリケーションがデータの局所性を制御する方法について説明する.

Spannerの展開はユニバースと呼ばれている.
Spannerがデータをグローバルに管理していることを考えると, 実行中のユニバースはほんの一握りになる.
現在, テスト/プレイグラウンドユニバース, 開発/プロダクションユニバース, およびプロダクション専用ユニーバスを実行している.

Spannerは一連のゾーンとして構成されており, 各ゾーンはBigtableサーバーの展開の大まかな類似点である.
ゾーンは, 管理展開の単位である.
ゾーンのセットは, データを複製できる場所のセットでもある.
ゾーンは, 新しいデータセンターがサービスを開始し, 古いデータセンターがオフになると, 実行中のシステムに追加または削除できる.
ゾーンは物理的な分離の単位でもある.
たとえば, 異なるアプリケーションのデータを同じデータセンター内の異なるサーバーセットに分散する必要がある場合など, データセンターには１つ以上のゾーンが存在する場合がある.

図1は, Spannerユニバースのサーバーを示している.
ゾーンには, 1つのzonemasterと100~数千のspanserverがある.
前者はデータをスパンサーバーに割り当てている. 後者はクライアントにデータを提供する.
ゾーンごとのロケーションプロキシは, データを提供するために割り当てられたスパンサーバーを見つけるためにクライアントによって使用されている.
ユニバースマスターと配置ドライバーは現在のシングルトンである.
ユニバースマスターは主に, 対話型デバッグのために全てのゾーンに関するステータス情報を表示するコンソールである.
配置ドライバーは, 定期的にスパンサーバーと通信して, 更新されたレプリケーションの制約を満たすめ, または負荷を分散するために, 移動する必要があるデータを見つける.
スペース上の理由から, spanserverの詳細のみを説明する.

2.1 Spanserver Software Stack
この章では, スパンサーバーの実装に焦点を当てて, Bigtableベースの実装にレプリケーションと分散トランザクションがどのように階層化されているかを説明する.
ソフトウェアスタックを図2に示す.
下部では, 各スパンサーバーがタブレットと呼ばれるデータ構造の100~1000のインスタンスを担当する.
タブレットは, 次のマッピングのバッグを実装するという点で, Bigtableのタブレット抽象化に似ている.

(key:string, timestamp:int64) → string

Bigtableとは異なり, Spannerはタイムスタンプをデータに割り当てる. これは, Spannerがキーバリューストアというよりも, マルチバージョンデータベースのような重要な方法である.
タブレットの状態は, Colossus(Google File Systemの後継)と呼ばれる分散ファイルシステムにあるBツリーのようなファイルと先行書き込みログのセットに保存される.

レプリケーションをサポートするために, 各スパンサーバーは各タブレットの上部に単一のPaxosステートマシンを実装する.
(初期のSpannerインカネーションは, タブレットごとに複数のPaxosステートマシンをサポートしていたため, より柔軟なレプリケーション構成が可能になった.
その設計の複雑さから, 私たちはそれを放棄することになった.)
各状態マシンは, そのメタデータを保存し, 対応するタブレットにログインする.
Paxosの実装は, 時間ベースのリーダールースで長命のリーダーをサポートする. リーダーリースの長さはデフォルトで10秒である.
現在のSpanner実装は, Paxosの書き込みを2回記録する.
タブレットのログに1回, Paxosのログに1回.
この選択は便宜上行われたものであり, 最終的には修正される可能性がある.
Paxosの実装にはパイプライン化されているため, WANの待ち時間がある場合にSpannerのスループットが向上する.
しかし, 書き込みはPaxosによって順番に適用される(四章では依存する事実).

Paxosステートマシンは, 一貫して複製されたマッピングのバッグを実装するために使用される.
各レプリカのキーと値のマッピング状態は, 対応するタブレットに保存される.
書き込みはリーダーでPaxosプロトコルを開始する必要がある.
十分に最新のレプリカで, 基盤となるタブレットからアクセス状態を直接読み取る.
レプリカのセットは, 集合的にPaxosグループである.

リーダーである全てのレプリカで, 各スパンサーバーはロックテーブルを実装して同時実行制御を実装する.
ロックテーブルには, 2フェーズロックの状態が含まれている.
キーの範囲をロック状態にマップする.
(ロックテーブルを効率的に管理するには, 長寿でPaxosリーダーを持つことが重要である.)
BigtableとSpannerの両方で, 競合が存在する場合の陥落的な同時実行制御ではパフォーマンスが低下する, 長時間のトランザクション(例えば, 数分程度かかるレポート生成など)向けに設計した.
トランザクション読み取りなどの同期を必要とする操作は, ロックテーブルでロックを取得する.
他の操作はロックテーブルをバイパスする.

リーダーである全てのレプリカで, 各スパンサーバーは, 分散トランザクションをサポートするトランザクションマネージャーも実装する.
トランザクションマネージャは, 参加者リーダーを実装するために使用される.
グループ内の他のレプリカは参加者レスーブト呼ばれている.
トランザクションが１つのPaxosグループのみを含む場合(ほとんどのトランザクションの場合), ロックテーブルとPaxosが一緒にトランザクション性を提供するため, トランザクションマネージャーをバイパスできる.
トランザクションに複数のPaxosグループが含まれる場合, それらのグループのリーダーは2フェーズコミットを実行するよう調整する.
参加者グループの１つがコーディネーターとして選択される.
そのグループの参加者リーダーはコーディネーターリーダーと呼ばれ, そのグループのレスーブはコーディネータースレーブと呼ばれる.
各トランザクションマネージャーの状態は, 基礎となるPaxosグループに格納される(したがって複製される).

2.2 Directories and Placement
キー値のマッピングの袋に加えて, Spanner実装は, ディレクトリと呼ばれるバケット化抽象化をサポートする. これは, 共通のプレフィックスを共有する連続したキーのセットである.
(用語ディレクトリの選択は歴史的な事故である. より適切な用語はバケットかもしれない.)
そのプレフィックスのソースについては2.3節で説明する.
ディレクトリをサポートすることにより, アプリケーションはキーを慎重に選択して, データの局所性を制御できる.

ディレクトリは, データ配列の単位である.
ディレクトリ内のすべてのデータのレプリケーション構成は同じである.
Paxosグループ間でデータを移動すると, 図3にしめすように, ディレクトリごとに移動する.
Spannerはディレクトリを移動してPaxosグループから負荷を削減する場合がある.
頻繁にアクセスさせるディレクトリを同じグループに入れる.
または, アクセサに近いグループにディレクトリを移動する.
クライアント操作の進行中にディレクトリを移動できる.
50MBのディレクトリを数秒で移動できると期待できる.

Paxosグループに複数のディレクトリが含まれている可能性があるということは, SpannerタブレットがBigtableタブレットと異なることを意味する.
前者は, 必ずしも行スペースの辞書式に連続した単一のパーティションではない.
代わりに, Spannerタブレットは, 行スペースの複数のパーティションをカプセル化できるコンテナである.
頻繁に一緒にアクセスされる複数のディレクトリを同じ場所に配置できるように, この決定を行った.

Movedirは, Paxosグループ間でディレクトリを移動するために使用されるグラウンドタスクである.
また, Movedirは, Paxosグループへのレプリカの追加または削除にも使用される. これは, SpannerがIn-Paxos構成の変更をまだサポートしていないためである.
かさばるデータ移動で進行中の読み取りと書き込みがブロックされないように, Movedirは単一のトランザクションとして実装されていない.
代わりに, movedirはデータの移動を開始しているという事実を登録し, データをバックグラウンドで移動している.
データのわずかな量を除いて全てを移動した場合, トランザクションを使用いてそのわずかな量を量子的に移動し, ２つのPaxosグループのメタデータを更新する.

ディレクトリは, アプリケーションによって地理的複製プロパティ(または略して配置)を指定できる最小単位でもある.
配置仕様言語の設計により, レプリケーション構成を管理する責任が分離された.
管理者は, レプリカの数とタイプ, およびレプリカの地理的配置という２つの次元を制御する.
これらは, これらの２つの次元で名前付きオプションのメニューを作成する(たとえば, 北米, １つの証人と５つの方法を複製).
アプリケーションは, 各データベースや個々のディレクトリにこれらのオプションを組みわわせてタグ付けすることにより, データの複製方法を制御する. 
例えば, アプリケーションは各エンドユーザーのデータを独自のディレクトリに保存する.
これにより, ユーザーAのデータはヨーロッパに３つのレプリカを持ち, ユーザーBのでデータは北米に５つのレプリカを持つことができる.

説明をわかりやすくするために, 単純化しすぎている.
実際, Spannerは, ディレクトリが大きくなりすぎると, ディレクトリを複製のフラグレントに分割する.
フラグメントは, 異なるPaxosグループ(および異なるサーバー)から提供される場合がある.
Movedirは実際には, ディレクトリ全体ではなく, グループ間でフラグメントを移動する.

2.3 Data Model
Spannerは, 次のデータ機能セットをアプリケーションに公開する.
スキーマ化された半リレーショナルテーブル, クエリ言語, および汎用トランザクションに基づくデータモデル.
これらの機能をサポートする動きは, 多くの要因によって推進された.
スキーマ化された半リレーショナルテーブルと同期レプリケーションサポートする必要性は, Negstoreの人気によってサポートされている.
Google内の少なくとも300のアプリケーションは, Megastoreを使用している(比較的低いパフォーマンスにも関わらず).
そのデータモデルはBigtableよりも管理が簡単であり, データセンター間で同期レプリケーションをサポートしているためである.
(Bigtableは, データセンター間で最終的に一貫したレプリケーションのみをサポートする.)
Megastoreを使用する有名なGoogleアプリケーションの例は, Gmail, Picasa, カレンダー, Androidマーケット, およびAppEngineである.
対話型データ分析ツールとしてDremelの人気を考えると, Spannerのクエリ言語のようなSQLをサポートする必要性も明らかであった.
最後に, Bigtableにはクロスロートランザクションがないため, 頻繁に苦情が寄せられた.
パーコレーターは, この失敗に対処するために部分的いん構築された.
一部の著者は, 一般的な2フェーズコミットは, パフォーマンスや可用性の問題が原因でサポートするには高すぎると主張している.
トランザクションの不足を常にコーディングするのではなく, ボトルネックが発生した時のトランザクションの過剰使用に起因するパフォーマンスの問題にアプリケーションプログラマーが対処する方が良いと考えている.
Paxosで2フェーズコミットを実行すると, 可用性の問題が軽減される.

アプリケーションデータモデルは, 実装でサポートされるディレクトリバケット化されたキーと値のマッピングの上に階層化される.
アプリケーションは, ユニバースに１つ以上のデータベースを作成する.
各データベースには, スキーマ化されたテーブルを無制限に含めることができる.
テーブルにはリレーショナルデータベーステーブルのように見え, 行, 列, バージョン付きの値を持つ.
Spannerのクエリ言語については詳しく説明しない.
Protocol-buffer-valuedフィールドをサポートする拡張機能を備えたSQLのように見える.

Spannerのデータモデルには単純にリレーショナルではなく, 行には名前が必要である.
より正確には, すべてのテーブルには, １つ以上の主キー列の順序付きセットが必要である.
この要件は, Spannerが依然としてキーバリューストアのように見える場所である.
主キーは行の名前を生成し, 各テーブルは主キー列から非主キー列へのマッピングを定義する.
行が存在するには, その値が(NULLであっても)行のキーに定義されている場合のみである.
この構造を課すことは, アプリケーションがキーの選択を通じてデータの局所性を制御できるため便利である.

図4に, ユーザーごと, アルバムごとに写真のメタデータを保存するためのSpannerスキーマの例を示している.
スキーマ言語はMegstoreの言語に似ているが, すべてのSpannerデータベースをクライアントが１つ以上のテーブル階層にパーディション化する必要があるという追加の要件がある.
クライアントアプリケーションは, INTERLEAVE IN 宣言を介してデータベーススキーマで階層を宣言する.
階層の最上部にあるテーブルはディレクトリテーブルである.
キーKを持つディレクトリテーブルの各行は, 辞書式順序でKで始まる子孫テーブルのすべての行とともに, ディレクトリを形成する.
ON DELETE CASCADEは, ディレクトリテーブルの行を削除すると, 関連するすべての子行が削除されることを示している.
この図は, サンプルデータベースンインターリーブレイアウトを示している.
例えば, Albums(2,1)は, ユーザーIDが2, アルバムIDが1のアルバムテーブルの行を表している.
ディレクトリを形成するためのテーブルのこのインターリープは, クライアントが複数のテーブル間に存在するローカリティ関連を記述することを可能にするために重要である.
これは, 断片化された分散データベースでの良好なパフォーマンスに必要である.
それがなければ, スパナーは最も重要な地域漢検を知らない.









