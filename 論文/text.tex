%\documentclass{jpsj3}
\documentclass[fp,twocolumn]{jpsj3}
%\documentclass[letter,twocolumn]{jpsj3}
%\documentclass[letterpaper,twocolumn]{jpsj3}
\usepackage{txfonts}

\makealetter
\@dblfptop 0pt
\makeatother

\renewcommand{\topfraction}{.85}
\renewcommand{\bottomfraction}{.60}
\renewcommand{\textfraction}{.15}
\renewcommand{\floatpagefraction}{.6}

\title{Derivaton of the QUBO formulation for sparse estimation}

\author{Tomohiro Yokota$^1$%\thanks{jpsj{\_}edit@jps.or.jp}
  , }
  
\inst{}

\abst{In recent years, annealing machine have been developed, and their use methods are considered in fields such as optimization problems and machine learning. In the annealing machine, it is necessary to express the problem to be dealt with in QUBO(Quadratic Unconstrained Binary Optimization) formulation and implement it as hardware. 
However, since the general method of rewriting to the QUBO format is not known yet, it needs to be derived individually. In this paper, we derive the QUBO formulation for the l1 norm (absolute value function) used in sparse estimation. As a result of experiment, it was possible to predict that one variable could be reduced from the result of numerical experiment by applying the Legendre transformation and Wolf-duality theorem to l1norm. By reviewing the formulation we were actually able to reduce one variable. In addition, as a result of conducting numerical experiments using the derived l1norm, it was confirmed that a sparse solution could be obtained.}

%%% Keywords are not needed any longer. %%%
%%% \kword{keyword1, keyword2, keyword3, \1dots} 
%%%

\begin{document}
\maketitle

\section{Introduction}
In recent yeaars, sepecial machines for annealing have benn developed such as D-Wave, digital annealer and CMOS annealing machine, and methods for use in various fields are being considered. The annealing machine acceptes Ising model parameters as input, but there are many functions thathave been represented yet because systematic derivation of the Ising model has been shown.
In the previous researches, the robust q-loss function is derived in the QUBO form using the Legendre transform, and its performance is evaluated using the classification problem as an example. Also, in the recently published papaer on the derivation of the ReLU function in the QUBO form, the derivation has made using the Wofle-duality theorem, so that the Legendre transformation alone is insufficient.
On the other hand, sparse estimation is used in the fields of image processing and machine learning, and it is an important research in reducing the number of data, selecting related data from high-dimensional and complex data and making it simple.
The purpose of this research is to derive an l1-norm, which is a regularization term of Lasso used in sparse estimation, in the QUBO form using the derivation method in [ReLUの論文番号].Furthemore, we evaluate whether the derived result is correct using simulated anneaing, and compare the results of Lasso(coordinate descent) and Lasso(simulated anneailng) to evaluate the performance. In the results, we verify that the estimate converges to zero.

\section{Background}
In this section, we describes the knowledge and algorithms used in the exeriment.

\subsection{QUBO and Ising model} %QUBOとIsingモデルについての説明
Since the QUBO formulation and the Ising model are equivalent, we can be converted to other form if we can be represented one side. The Ising model is represented as follows:
\begin{}
\end{}
where $\sigma_{i}\in {-1,+1}$is a spin variable for $i$-th spin, $J_{ij}\in \mathbb{R}$ a quadratic term of $i$ and $j$, and $h_{i}\in \mathbb{R}$ a liner term of $i$. We can easily converted the Ising model to QUBO formulation, which uses binary variable $q_{i}\in {0,1}$, by applying $q_{i}=\frac{\sigma_{i}+1}{2}$. 

\subsection{Simulated Annealing} %実験で利用するSAアルゴリズムについての説明
Simulated annealing(SA) is optimization algorithm, and it find the minimum of the objective function efficiently. SA repeatedly minimize the objective function by moving the variables randmoly (in this papaer, it moves at a fixed size) at each iteration. Also, it does not necessarily reject non-minimizing movement, but accepts it with a certain acceptance probability. It is possible to avoid convergence to the local minimum. Here, the acceptance probability is inversely proportional to the inverse temperature (execution time) and the size of the energy difference before and after movement. \cite{} show the SA algorithm.

\begin{}
SA algorithm
\end{}

\subsection{Lasso} %Lassoについての説明　コスト関数とその特性など
Lasso(Least Absolute Shrinkage and Selection Operator) is a method to estimate regression coefficients sparsely by using l1-norm regularization terms. The cost function of Lasso is represented as follows:
\begin{}
\end{}
where $$. Here, since the Lasso regularizationt term is not continuous, updating by partial differentation with respect to parameter $\beta$ can not be obtained. So, in the next section, we will explain how to obtain the update formula of $\beta$ using the Coordinate Descent.

\subsubsection{Cardininay Decsent} %比較実験で利用するうLassoアルゴリズムについての説明
Coordinate Descent is an algorithm, which updates by repeating the procedure of fixing the values of $p-1$ elements among $p$ parameters and updating only one remaining parameter for all elements.

\section{Derivation of l1-norm in QUBO formulation} %l1-normのQUBOでの定式化
We define the function $f(m)$ as follows:
\begin{}
  f(m)=-\min{(-m,m)}
\end{}
A function form of $f(m)$ is shown in Fig. We convert $f(m)$ to quadratic form by applying the Legendre transform to it. Using the method used in the papar[], the one converted to quadratic form is represented as follows:
\begin{}
  F(m)=-\min_{t}{\{-mt\}} \ \text{subject} \ \text{to} \ \-1 \leq t \leq 1
\end{}
We could express the quadratic form of $f(m)$ as (式), but there is the min function is proceded by a minus sign, which makes it difficult to solve an optimization problem that combines multiple cost functions. Let the other cost function be $C(m)$, and the combination with $F(m)$ is as follows:
\begin{}
\end{}
Hence, it is not in the form of minimization problem for both $m$ and $t$.
In precious researche [], this problem was solved by applying Wolfe dual theorem to (式). By applying this, the dual problem of $F(m)$ becomes as follows:
\begin{}
\end{}


\subsection{Results} %定式化したものを利用して実験を行う

\section{Rewriting derived formula} %定式化の見直し
We can think of the following from the results of the numerical experiments in the previous section: The variable $t$ seems to be taking a random value rather than settling to the optimal value, so we will consider whether we can eliminate $t$ by reviewing the formulation. 
The cost function can be transformed as follows using constraint condition.
\begin{}
\end{}

\subsectoin{Results} %定式化したものを利用して実験を行う
The result of experimenting the optimization problem under the same experimental conditions as section() with (式) as the objective function is as shown in Fig.(). From this result, we seem that removing the variable $t$ does not affect the result. 

\section{Comparative experiment} %Lassoについて, 比較実験を行う
\subsection{Results} %その時の比較結果を示し, 

\begin{acknowledgment}

%\acknowledgment

%For enveironments for acknowledgment(s) are available: \verb|acknowledgment|, \verb|acknowledgments|, \verb|acknowledgment|, and \verb|acknowledgments|.

\end{acknowledgment}

%\appndix
%\section{}
%Use the \verb|\appendix| command if you need an appendix(es). The \verb|\section| command should follow even though there is no title for the appendix (see above in the source of this file).
%For authurs of Invited Review Papers, the \verb|profile| command si prepared for the author(s)' profile. A simple example is shown below.

%\begin{verbatim}
%\profile{Taro Butsuri}{was born in Tokyo, Japan in 1965. ...}
%\end{verbatim}

\begin{thebibliography}{1}
\bibitem{name1}
\bibitem{name2}
\bibitem{name3}
\end{thebibliography}

\end{document}




